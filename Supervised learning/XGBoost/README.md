
# ğŸš€ XGBoost Projects â€” Supervised Learning  
### Gradient Boosting â€¢ Tabular Data â€¢ Binary Classification

This folder contains my implementations of **XGBoost**, one of the most powerful and widely-used algorithms for structured data.  
All projects here demonstrate full ML pipelines, including:

- Data cleaning & preprocessing  
- Feature engineering  
- Train/test splitting  
- Model training and evaluation  
- ROCâ€“AUC, confusion matrix, classification report  
- Prediction pipelines  
- Exported, production-ready models (`.pkl`)  

Below are the projects included in this directory.

---

# ğŸ“Œ 1. Telco Customer Churn Prediction  
**Predicting whether a telecom customer will leave (churn) based on contract details, billing behavior, demographics, and service usage.**

### ğŸ”¹ Highlights  
- Full preprocessing with 1,178 engineered features  
- Hyperparameter-tuned XGBoost model  
- ROCâ€“AUC: **0.77**  
- Balanced accuracy: **77%**  
- Exported as `XGboost_model.pkl`  
- Tree visualization included (`xgb_tree.png`)

ğŸ”— **Project Link:**  
https://github.com/Pinkkygold/Machine-Learning-projects/tree/main/Supervised%20learning/XGBoost/TeleCustomerPred

---

# ğŸ“Œ 2. Titanic Survival Prediction  
**Classifying whether a Titanic passenger survived based on age, gender, class, family size, fare, embarkationâ€”and an engineered age category feature.**

### ğŸ”¹ Highlights  
- Cleaned + encoded dataset (after removing duplicates and missing values)  
- New feature: **Age_Category**  
- XGBoost classifier with strong performance  
- Accuracy: **78.8%**  
- ROCâ€“AUC: **0.84**  
- Exported model: `TitanicXGB.pkl`  

ğŸ”— **Project Link:**  
https://github.com/Pinkkygold/Machine-Learning-projects/tree/main/Supervised%20learning/XGBoost/TitanicPred

---

## ğŸ§  Why XGBoost?

- Excellent performance on tabular datasets  
- Handles missing values internally  
- Regularization to reduce overfitting  
- Fast training and scalable  
- Great compatibility with pandas & NumPy  
- Flexible evaluation metrics  

This folder showcases how XGBoost can be applied to **real-world business problems (customer churn)** and **classic analytical datasets (Titanic)** with end-to-end ML workflows.

---

## ğŸ‘¨â€ğŸ’» Author

**Awab Elkhair Abdalla**  
Machine Learning Engineer | Electronics Engineer  

- ğŸ”— LinkedIn: https://www.linkedin.com/in/awab-abdalla  
- ğŸ™ GitHub: https://github.com/Pinkkygold  
- ğŸ“§ Email: awab1355@gmail.com

